<!DOCTYPE html>
    <html>

    <head>

        <meta charset="UTF-8">
        <meta name="description" content="Data Science Capstone">
        <meta name="keywords" content="Data, Science, Python, Pandas, Coursera, Capstone">
        <meta name="author" content="Raymond Cool">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <style>
                div.a {
                text-indent: 50px;
                }
                .img-container {
                text-align: center;
                }
                
            </style>
    </head>
    <body>

<h1> Predicting Severe Traffic Disruption For Use in Preventing Traffic Disruption</h1>


<h2> About the Method</h2><br>
<h3><em>“We adopted KNN in the end because it was much more reliable in reproduction,
         and it provided the best tolerance for a single dominant feature in the Severity 
         grade distribution.”</em></h3>

<div class="a">
    <p>
        To begin we looked at the overall distribution of accidents throughout the state. 
        The most populous of Delaware's three counties is Newcastle. It is also a major 
        transportation hub for interstate travel because it hosts a very important section 
        of the I-95 corrider stretching almost the entire length of the East Coast. 
        Understanding the chaos inherent to volumous travel scenarios is challenging to 
        say the least. And our dataset, while selected for it's comprehensiveness, has too 
        much dimension taken whole cloth, to be of any benefit.
        <br><br>
    </p>

        <div class="img-container">
            <img src="https://www.stainlessray.com/Images/distribution_severity_grades.png" 
            alt="coordinate_heat_map" width=65%>
        </div>

</div>

<div class="a">
    <p>
        So when all of those data are encoded and fit for modeling, the propensity for overfit 
        jumps out at you. Combining a single "General" or non-specific datapoint (eg "Zipcode"), 
        was effective at taming the model, but no where near accurate enough to be useful in 
        prediction. The distribution of severity grades also give way to some new inferences 
        about "Conditions" as a predictor. So we bagan our machine learning exploration with 
        the goal to find out what we could do about the initial take that - This data is too 
        detailed!   
      <br><br>

    </p>
</div>

<div class="a">
    <p>
        We tested with a broad base of prediction algorithms. Some of them were: Support Vector 
        Machine (SVM), Decision Tree, Logistic Regression, and finally K Nearest Neighbor 
        classification (KNN). We adopted KNN in the end because it was much more reliable in 
        reproduction, and it provided the best tolerance for a single dominant feature in the 
        Severity grade distribution. The second tier severity quantifies accidents that are 
        individually, not very impacftful on road conditions. However, the tier two accident
        represents over two thirds of the datapoints in the sample. Why is that a problem? 
        Looking at the predictions, several of the models took the easy way to "high accuracy". 
        They just predicted tier two exclusively. This lead to the appearance of extremely high 
        accuracy, but exceedingly high logloss. It wasn't predicting anything. It decided 
        not to play the game.
      <br><br>

    </p>
</div>




    

</body>
</html>